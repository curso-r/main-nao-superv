---
title: "Modelagem Não Supervisionada"
subtitle: "Redução de dimensionalidade"
author: "<img src = 'https://d33wubrfki0l68.cloudfront.net/9b0699f18268059bdd2e5c21538a29eade7cbd2b/67e5c/img/logo/cursor1-5.png' width = '30%'>"
date: "29/08/2022"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "css/xaringan-themer.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "4:3"
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(ggplot2)
library(magrittr)
library(knitr)
library(tidyverse)
library(ISLR)
library(kableExtra)

link <- function(href, ...) {
  htmltools::a(
    href = href,
    target = "_blank",
    ...
  )
}
  
theme_set(theme_minimal(14))
options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE,
  fig.width=6, 
  fig.height=6,
  fig.align='center'
)
```

class: middle, center

# Redução de dimensionalidade

---
# Ideia geral

- Redução de dimensionalidade é um termo que se usa normalmente para falar da redução do número de colunas de uma base.

    - Queremos ir das dimensões $n \times p$ para $n \times d$, com $d<p$.
    
- Motivação:
    
    - Frequentemente temos colunas com mais ou menos a mesma informação que poderiam ser resumidas em apenas uma delas ou talvez em um resumo das duas (como uma média).
    
    - Quando $d=2$ temos a agradável situação em que $n \times p$ vira uma tabela que pode ser visualizada por um gráfico de dispersão.
    
    - Em algumas situações, uma análise nas dimensões $n \times d$ é melhor do que uma análise em $n \times p$, como por exemplo uma clusterização ou modelo estatístico.

---
# Ideia geral

- A Análise de Componentes Principais é um algoritmo de **redução de dimensionalidade**

- Digamos que os nossos dados sejam representados pela matriz $X$, com $n$ linhas e $k$ colunas:

$$X = \begin{bmatrix}x_{11}  \ x_{12} \ \dots \ x_{1k} \\ x_{21} \ x_{22} \ \dots \ x_{2k} \\ x_{i1} \ x_{i2} \ \dots x_{ik} \\ x_{n1} \ x_{n2} \dots \ x_{nk}\end{bmatrix}$$
- A ideia do PCA é encontrar uma matriz $Y$ com $k$ linhas e $d$ colunas, onde $d$ é menor do que que $k$, que capte características importantes de $X$.

- O processo de encontrar $Y$ consiste em usar essa matriz para tentar "reconstruir" $X$. Se a reconstrução for bem sucedida (tiver erro baixo), então podemos usar $Y$ ao invés de $X$ para várias coisas: modelagem, visualização, interpretação etc.

---

# Ideia geral

- A fómula da reconstrução usada no PCA é conta com uma matriz auxiliar $A$:

$$X' = Y A+\mu$$

$$X = \begin{bmatrix}x_{11}  \ x_{12} \ \dots \ x_{1k} \\ x_{21} \ x_{22} \ \dots \ x_{2k} \\ x_{i1} \ x_{i2} \ \dots x_{ik} \\ x_{n1} \ x_{n2} \dots \ x_{nk}\end{bmatrix} = \begin{bmatrix}y_{11}  \ y_{12} \ \dots \ y_{1d} \\ y_{21} \ y_{22} \ \dots \ y_{2d} \\ y_{i1} \ y_{i2} \ \dots y_{id} \\ y_{n1} \ y_{n2} \dots \ y_{nd}\end{bmatrix} \times \begin{bmatrix} a_{11} \ \dots a_{1k} \\ \vdots \ \ddots \ \vdots \\a_{d1} \ \dots a_{dk}\end{bmatrix} + \mu$$
---

# Ideia geral

- Quando $k = d = 2$ podemos visualizar o que está acontecendo. Em termos menos matemáticos, o que a fórmula acima expressa é uma mudança de base da matriz $X$. Estamos tentando escrever cada linha de $X$ como combinações lineares dos vetores coluna de $A$. No plano, queremos encontrar as setas abaixo:

```{r, echo = FALSE, out.width="50%"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1200px-GaussianScatterPCA.svg.png")
```

---

# Ideia geral

$$X = \begin{bmatrix}x_{11}  \ x_{12}\\ x_{21} \ x_{22}  \\ x_{i1} \ x_{i2}  \\ x_{n1} \ x_{n2}\end{bmatrix} = \begin{bmatrix}y_{11}  \ y_{12}\\ y_{21} \ y_{22}  \\ y_{i1} \ y_{i2}  \\ y_{n1} \ y_{n2}\end{bmatrix} \times \begin{bmatrix}a_{11}  \ a_{12}\\ a_{21} \ a_{22}  \end{bmatrix} + \mu$$

$$(x_{11}, x_{12}) = (y_{11} \times a_{11} + y_{12} \times a_{21}, y_{11} \times a_{12} + y_{12} \times a_{22}) + \mu$$

---

# Ideia geral

Toda a mágica por trás do PCA consiste no fato de que muitas vezes podemos descartar as segundas parcelas na fórmula abaixo (por conta de um erro baixo):

$$\begin{bmatrix}x_{11}  \ x_{12}\\ x_{21} \ x_{22}  \\ x_{i1} \ x_{i2}  \\ x_{n1} \ x_{n2}\end{bmatrix} \approx \begin{bmatrix}y_{11}  \\ y_{21}   \\ y_{i1} \\ y_{n1} \end{bmatrix} \times \begin{bmatrix}a_{11} \ a_{12} \end{bmatrix} + \mu$$

$$(x_{11}, x_{12}) = (y_{11} \times a_{11} + y_{12} \times a_{21}, y_{11} \times a_{12} + y_{12} \times a_{22}) + \mu$$
$$(x_{11}, x_{12}) \approx (y_{11} \times a_{11}, y_{11} \times a_{12}) + \mu$$

A equação acima é a equação de uma reta!

Vamos ao R.

---

# Não linearidade

- A fórmula de aproximação do PCA é linear, ou seja, ela é uma equação que busca incógnitas para plugar em uma equação que faz produtos e somas.

$$X = T \times A + \mu$$
- Isso é muito bom em vários contextos, mas as aproximações proporcionadas pelo PCA simplesmente não são úteis na prática.

---

# Não linearidade

- As técnicas da família SNE buscam encontrar o que se chama de Stochastic Neighbor Embedding, que essencialmente parte da seguinte ideia:

É útil imaginar que os pontos são posicionados de acordo com um sorteio:

$$\text{Sorteia }x_1\rightarrow\text{Sorteia }x_2\text{ a partir de }x_1 \rightarrow \text{Sorteia }x_3\text{ a partir de }x_2$$
A probabilidade de encontramos um $x_2$ a partir de um $x_1$ e de um $x_3$ a partir de $x_2$ da distância entre os pontos. Também existe uma chance de um ponto ser muito distante dos anteriores, e esse número é chamado de "perplexidade".

A ideia por trás do tSNE é procurar por uma distribuição que leve $x_j$ a $x_i$ em dimensão baixa que produza uma matriz de distâncias _similar_ a matriz de distâncias de dimensão alta.

Vamos ao R!
